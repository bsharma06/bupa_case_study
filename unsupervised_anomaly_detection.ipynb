{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unsupervised Anomaly Detection\n",
        "\n",
        "This notebook demonstrates unsupervised anomaly detection using the Isolation Forest algorithm on the `payments_master.csv` dataset. The objective is to identify unusual patterns or outliers within the payment data that could indicate potential errors, process deviations, or fraudulent activities without requiring pre-labeled fraud cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Preparation\n",
        "\n",
        "We load the `payments_master.csv` dataset. Date columns are converted to datetime objects, and `time_to_payment` is calculated as a feature. Missing values in numerical columns are filled with the median, and categorical columns with the mode. These steps ensure the data is clean and suitable for the anomaly detection model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "try:\n",
        "    df_payments = pd.read_csv('payments_master.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: payments_master.csv not found. Please ensure the file is in the correct directory.\")\n",
        "    exit()\n",
        "\n",
        "# Convert date columns to datetime objects for potential feature engineering\n",
        "date_cols = ['Date received', 'Date of invoice', 'Date of authorisation', 'Payment due date', 'Date of payment']\n",
        "for col in date_cols:\n",
        "    if col in df_payments.columns:\n",
        "        df_payments[col] = pd.to_datetime(df_payments[col], errors='coerce')\n",
        "\n",
        "# Calculate 'time to payment' as a feature\n",
        "df_payments['time_to_payment'] = (df_payments['Date of payment'] - df_payments['Date received']).dt.days\n",
        "\n",
        "# Drop original date columns as we have 'time_to_payment'\n",
        "df_payments = df_payments.drop(columns=date_cols, errors='ignore')\n",
        "\n",
        "# Handle missing values: fill numerical with median, categorical with mode\n",
        "for col in ['Invoice value', 'Payment amount', 'time_to_payment']:\n",
        "    if col in df_payments.columns:\n",
        "        df_payments[col].fillna(df_payments[col].median(), inplace=True)\n",
        "\n",
        "for col in ['Research team', 'Type of expense', 'Company', 'Payment Status', 'Submitted by', 'Authorised by', 'Payment authoriser']:\n",
        "    if col in df_payments.columns:\n",
        "        df_payments[col].fillna(df_payments[col].mode()[0], inplace=True)\n",
        "\n",
        "print(f\"Dataset loaded with {len(df_payments)} records after preprocessing.\")\n",
        "print(df_payments.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Selection and Preprocessing\n",
        "\n",
        "We select a set of features that are likely to exhibit unusual patterns in the case of anomalies. These include numerical features such as 'Invoice value', 'Payment amount', and 'time_to_payment', as well as various categorical features related to the transaction details.\n",
        "\n",
        "Similar to previous notebooks, we use `ColumnTransformer` to apply `StandardScaler` to numerical features and `OneHotEncoder` to categorical features, ensuring the data is properly scaled and encoded for the anomaly detection model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = ['Invoice value', 'Payment amount', 'time_to_payment', \n",
        "            'Research team', 'Type of expense', 'Company', \n",
        "            'Payment Status', 'Submitted by', 'Authorised by', 'Payment authoriser']\n",
        "\n",
        "# Ensure all features exist in df_payments, filter if not\n",
        "features = [f for f in features if f in df_payments.columns]\n",
        "\n",
        "X_unsupervised = df_payments[features]\n",
        "\n",
        "# Identify categorical and numerical features for preprocessing\n",
        "categorical_features = ['Research team', 'Type of expense', 'Company', 'Payment Status', 'Submitted by', 'Authorised by', 'Payment authoriser']\n",
        "numerical_features = ['Invoice value', 'Payment amount', 'time_to_payment']\n",
        "\n",
        "# Filter to only include features actually present in the dataframe\n",
        "categorical_features = [f for f in categorical_features if f in X_unsupervised.columns]\n",
        "numerical_features = [f for f in numerical_features if f in X_unsupervised.columns]\n",
        "\n",
        "# Preprocessing pipelines for numerical and categorical features\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor_unsupervised = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "print(f\"Selected {len(features)} features for unsupervised anomaly detection.\")\n",
        "print(\"Categorical features:\", categorical_features)\n",
        "print(\"Numerical features:\", numerical_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Anomaly Detection with Isolation Forest\n",
        "\n",
        "We use the Isolation Forest algorithm, an effective unsupervised method for anomaly detection. This model works by isolating observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. Repeatedly splitting the data in this way creates 'isolation trees'. Anomalies are instances that require fewer splits to be isolated.\n",
        "\n",
        "The `contamination` parameter is set to 0.05, estimating that 5% of our data are anomalies. The model is then fitted to the preprocessed data, and anomaly scores and labels are generated. Lower anomaly scores indicate a higher likelihood of being an outlier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the Isolation Forest model pipeline\n",
        "# contamination is the proportion of outliers in the dataset (estimate)\n",
        "model_isolation_forest = Pipeline(steps=[('preprocessor', preprocessor_unsupervised),\n",
        "                                         ('detector', IsolationForest(random_state=42, contamination=0.05))]) # Assuming 5% outliers\n",
        "\n",
        "# Fit the model and make predictions (anomaly scores and labels)\n",
        "model_isolation_forest.fit(X_unsupervised)\n",
        "\n",
        "anomaly_scores = model_isolation_forest.decision_function(X_unsupervised)\n",
        "anomaly_predictions = model_isolation_forest.predict(X_unsupervised) # -1 for outliers, 1 for inliers\n",
        "\n",
        "df_payments['anomaly_score'] = anomaly_scores\n",
        "df_payments['is_anomaly'] = anomaly_predictions\n",
        "\n",
        "outliers_unsupervised = df_payments[df_payments['is_anomaly'] == -1]\n",
        "\n",
        "print(f\"\\n--- Unsupervised Anomaly Detection (Isolation Forest) ---\")\n",
        "print(f\"Identified {len(outliers_unsupervised)} anomalies using Isolation Forest (assuming 5% contamination).\")\n",
        "print(\"\\nTop 10 anomalies (lowest anomaly score):\")\n",
        "print(outliers_unsupervised.sort_values(by='anomaly_score').head(10)[['Invoice number', 'Invoice value', 'time_to_payment', 'anomaly_score']])\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
