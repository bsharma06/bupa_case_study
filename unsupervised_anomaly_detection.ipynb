{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unsupervised Anomaly Detection\n",
        "\n",
        "Unsupervised anomaly detection using the Isolation Forest algorithm on the `payments_master.csv` dataset. The objective is to identify unusual patterns or outliers within the payment data that could indicate potential errors, process deviations, or fraudulent activities without requiring pre-labeled fraud cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Preparation\n",
        "\n",
        "We load the `payments_master.csv` dataset. Date columns are converted to datetime objects, and `time_to_payment` is calculated as a feature. Missing values in numerical columns are filled with the median, and categorical columns with the mode. These steps ensure the data is clean and suitable for the anomaly detection model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data directory is set to: c:\\Users\\homep\\Documents\\Projects\\bupa_case_study\\data\n",
            "************************\n",
            "\n",
            "Payment master path: c:\\Users\\homep\\Documents\\Projects\\bupa_case_study\\data\\payments_master.csv\n",
            "Fraud case master path: c:\\Users\\homep\\Documents\\Projects\\bupa_case_study\\data\\fraud_cases_master.csv\n",
            "Research team master path: c:\\Users\\homep\\Documents\\Projects\\bupa_case_study\\data\\research_team_master.csv\n",
            "Research team member master path: c:\\Users\\homep\\Documents\\Projects\\bupa_case_study\\data\\research_team_member_master.csv\n"
          ]
        }
      ],
      "source": [
        "cwd = os.getcwd()\n",
        "data_dir = os.path.join(cwd, 'data')\n",
        "print(f\"Data directory is set to: {data_dir}\")\n",
        "\n",
        "payment_master_path = os.path.join(data_dir, 'payments_master.csv')\n",
        "fraud_case_master_path = os.path.join(data_dir, 'fraud_cases_master.csv')\n",
        "research_team_master_path = os.path.join(data_dir, 'research_team_master.csv')\n",
        "research_team_member_master_path = os.path.join(data_dir, 'research_team_member_master.csv')\n",
        "print(\"************************\\n\")\n",
        "print(f\"Payment master path: {payment_master_path}\")\n",
        "print(f\"Fraud case master path: {fraud_case_master_path}\")\n",
        "print(f\"Research team master path: {research_team_master_path}\")\n",
        "print(f\"Research team member master path: {research_team_member_master_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: payments_master.csv not found. Please ensure the file is in the correct directory.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'df_payments' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m date_cols = [\u001b[33m'\u001b[39m\u001b[33mDate received\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDate of invoice\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDate of authorisation\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPayment due date\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDate of payment\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m date_cols:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf_payments\u001b[49m.columns:\n\u001b[32m     12\u001b[39m         df_payments[col] = pd.to_datetime(df_payments[col], errors=\u001b[33m'\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Calculate 'time to payment' as a feature\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'df_payments' is not defined"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "try:\n",
        "    df_payments = pd.read_csv(payment_master_path)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: payments_master.csv not found. Please ensure the file is in the correct directory.\")\n",
        "    exit()\n",
        "\n",
        "# Convert date columns to datetime objects for potential feature engineering\n",
        "date_cols = ['Date received', 'Date of invoice', 'Date of authorisation', 'Payment due date', 'Date of payment']\n",
        "for col in date_cols:\n",
        "    if col in df_payments.columns:\n",
        "        df_payments[col] = pd.to_datetime(df_payments[col], errors='coerce')\n",
        "\n",
        "# Calculate 'time to payment' as a feature\n",
        "df_payments['time_to_payment'] = (df_payments['Date of payment'] - df_payments['Date received']).dt.days\n",
        "\n",
        "# Drop original date columns as we have 'time_to_payment'\n",
        "df_payments = df_payments.drop(columns=date_cols, errors='ignore')\n",
        "\n",
        "# Handle missing values: fill numerical with median, categorical with mode\n",
        "for col in ['Invoice value', 'Payment amount', 'time_to_payment']:\n",
        "    if col in df_payments.columns:\n",
        "        df_payments[col].fillna(df_payments[col].median(), inplace=True)\n",
        "\n",
        "for col in ['Research team', 'Type of expense', 'Company', 'Payment Status', 'Submitted by', 'Authorised by', 'Payment authoriser']:\n",
        "    if col in df_payments.columns:\n",
        "        df_payments[col].fillna(df_payments[col].mode()[0], inplace=True)\n",
        "\n",
        "print(f\"Dataset loaded with {len(df_payments)} records after preprocessing.\")\n",
        "print(df_payments.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Selection and Preprocessing\n",
        "\n",
        "We select a set of features that are likely to exhibit unusual patterns in the case of anomalies. These include numerical features such as 'Invoice value', 'Payment amount', and 'time_to_payment', as well as various categorical features related to the transaction details.\n",
        "\n",
        "Similar to previous notebooks, we use `ColumnTransformer` to apply `StandardScaler` to numerical features and `OneHotEncoder` to categorical features, ensuring the data is properly scaled and encoded for the anomaly detection model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = ['Invoice value', 'Payment amount', 'time_to_payment', \n",
        "            'Research team', 'Type of expense', 'Company', \n",
        "            'Payment Status', 'Submitted by', 'Authorised by', 'Payment authoriser']\n",
        "\n",
        "# Ensure all features exist in df_payments, filter if not\n",
        "features = [f for f in features if f in df_payments.columns]\n",
        "\n",
        "X_unsupervised = df_payments[features]\n",
        "\n",
        "# Identify categorical and numerical features for preprocessing\n",
        "categorical_features = ['Research team', 'Type of expense', 'Company', 'Payment Status', 'Submitted by', 'Authorised by', 'Payment authoriser']\n",
        "numerical_features = ['Invoice value', 'Payment amount', 'time_to_payment']\n",
        "\n",
        "# Filter to only include features actually present in the dataframe\n",
        "categorical_features = [f for f in categorical_features if f in X_unsupervised.columns]\n",
        "numerical_features = [f for f in numerical_features if f in X_unsupervised.columns]\n",
        "\n",
        "# Preprocessing pipelines for numerical and categorical features\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Create a preprocessor using ColumnTransformer\n",
        "preprocessor_unsupervised = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "print(f\"Selected {len(features)} features for unsupervised anomaly detection.\")\n",
        "print(\"Categorical features:\", categorical_features)\n",
        "print(\"Numerical features:\", numerical_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Anomaly Detection with Isolation Forest\n",
        "\n",
        "We use the Isolation Forest algorithm, an effective unsupervised method for anomaly detection. This model works by isolating observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. Repeatedly splitting the data in this way creates 'isolation trees'. Anomalies are instances that require fewer splits to be isolated.\n",
        "\n",
        "The `contamination` parameter is set to 0.05, estimating that 5% of our data are anomalies. The model is then fitted to the preprocessed data, and anomaly scores and labels are generated. Lower anomaly scores indicate a higher likelihood of being an outlier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the Isolation Forest model pipeline\n",
        "# contamination is the proportion of outliers in the dataset (estimate)\n",
        "model_isolation_forest = Pipeline(steps=[('preprocessor', preprocessor_unsupervised),\n",
        "                                         ('detector', IsolationForest(random_state=42, contamination=0.05))]) # Assuming 5% outliers\n",
        "\n",
        "# Fit the model and make predictions (anomaly scores and labels)\n",
        "model_isolation_forest.fit(X_unsupervised)\n",
        "\n",
        "anomaly_scores = model_isolation_forest.decision_function(X_unsupervised)\n",
        "anomaly_predictions = model_isolation_forest.predict(X_unsupervised) # -1 for outliers, 1 for inliers\n",
        "\n",
        "df_payments['anomaly_score'] = anomaly_scores\n",
        "df_payments['is_anomaly'] = anomaly_predictions\n",
        "\n",
        "outliers_unsupervised = df_payments[df_payments['is_anomaly'] == -1]\n",
        "\n",
        "print(f\"\\n--- Unsupervised Anomaly Detection (Isolation Forest) ---\")\n",
        "print(f\"Identified {len(outliers_unsupervised)} anomalies using Isolation Forest (assuming 5% contamination).\")\n",
        "print(\"\\nTop 10 anomalies (lowest anomaly score):\")\n",
        "print(outliers_unsupervised.sort_values(by='anomaly_score').head(10)[['Invoice number', 'Invoice value', 'time_to_payment', 'anomaly_score']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"crest\")\n",
        "\n",
        "# --- 1️⃣ Anomaly score distribution ---\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(df_payments['anomaly_score'], bins=30, kde=True)\n",
        "plt.title('Distribution of Anomaly Scores')\n",
        "plt.xlabel('Anomaly Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 2️⃣ Scatter plot: Invoice value vs Payment amount ---\n",
        "if {'Invoice value', 'Payment amount'}.issubset(df_payments.columns):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(\n",
        "        data=df_payments,\n",
        "        x='Invoice value',\n",
        "        y='Payment amount',\n",
        "        hue='is_anomaly',\n",
        "        palette={1: 'green', -1: 'red'},\n",
        "        alpha=0.7\n",
        "    )\n",
        "    plt.title('Invoice Value vs Payment Amount (Anomalies Highlighted)')\n",
        "    plt.legend(title='Anomaly', labels=['Normal', 'Anomaly'])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- 3️⃣ Time to Payment vs Invoice Value (to detect delay-related anomalies) ---\n",
        "if {'time_to_payment', 'Invoice value'}.issubset(df_payments.columns):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(\n",
        "        data=df_payments,\n",
        "        x='time_to_payment',\n",
        "        y='Invoice value',\n",
        "        hue='is_anomaly',\n",
        "        palette={1: 'blue', -1: 'orange'},\n",
        "        alpha=0.7\n",
        "    )\n",
        "    plt.title('Time to Payment vs Invoice Value')\n",
        "    plt.xlabel('Time to Payment (days)')\n",
        "    plt.ylabel('Invoice Value')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- 4️⃣ Feature importance proxy: correlation heatmap ---\n",
        "if len(numerical_features) > 1:\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    corr = df_payments[numerical_features].corr()\n",
        "    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "    plt.title('Correlation between Numerical Features')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- 5️⃣ Count of anomalies by company or type of expense ---\n",
        "if 'Company' in df_payments.columns:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.countplot(data=df_payments, x='Company', hue='is_anomaly', palette={1: 'green', -1: 'red'})\n",
        "    plt.title('Anomalies by Company')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if 'Type of expense' in df_payments.columns:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.countplot(data=df_payments, x='Type of expense', hue='is_anomaly', palette={1: 'green', -1: 'red'})\n",
        "    plt.title('Anomalies by Type of Expense')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
